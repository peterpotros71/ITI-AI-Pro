{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01: PySpark UDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* User-Defined Function (UDF) \n",
    "* most useful feature of Spark SQL & DataFrame that is used to extend the PySpark build in capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can extend RDBMS server by adding a new function that behaves just like built-in function, such as abs() or concat().\n",
    "\n",
    "these functions need to register in the database library and use them on SQL as regular functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need UDF?\n",
    "\n",
    "we are using parallel processing, then if we use normal for-loops we will need to iterate along all records in iterative manner. \n",
    "\n",
    "but when we use udf then the operations are independent to each other  \n",
    "\n",
    "for example: we need to create a new column with string values from another column in reverse order, there's no built-in function to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you create any UDF, do your research to check if the similar function you wanted is already available in Spark SQL Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are creating UDFâ€™s you need to design them very carefully otherwise you will come across optimization & performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Data Frame DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/25 22:30:33 WARN Utils: Your hostname, fou4d-ubuntu resolves to a loopback address: 127.0.0.1; using 192.168.1.156 instead (on interface wlp8s0)\n",
      "21/09/25 22:30:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/mohamed/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/25 22:30:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Lab01').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "|ID |Name           |\n",
      "+---+---------------+\n",
      "|1  |Ali mohamed    |\n",
      "|2  |saadya Mahmoud |\n",
      "|3  |Mohamed mahmoud|\n",
      "|4  |aliaa eldemery |\n",
      "+---+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columns = [\"ID\",\"Name\"]\n",
    "data = [(\"1\", \"Ali mohamed  \"),\n",
    "    (\"2\", \"saadya Mahmoud\"),\n",
    "    (\"3\", \"Mohamed mahmoud\"),\n",
    "    (\"4\", \"aliaa eldemery\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to modify names to be in proper format, capitalize first letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Python Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCase(str):\n",
    "    resultingStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "        resultingStr = resultingStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    resultingStr = resultingStr.strip()\n",
    "    return resultingStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Small Letters'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it's just a normal simple python function!\n",
    "convertCase(\"small letters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert a Python function to PySpark UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert this function convertCase() to UDF by passing the function to PySpark SQL udf(), this function is available at org.apache.spark.sql.functions.udf package. Make sure you import this package before using it.\n",
    "\n",
    "PySpark SQL udf() function returns *org.apache.spark.sql.expressions.UserDefinedFunction* class object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import org.apache.spark.sql.functions.udf package\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspark udf() function. \n",
    "\n",
    "It takes 2 arguments, the custom function and the returned datatype (the data type of value returned by custom function. It is **StringType()** by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertUDF = udf(lambda z: convertCase(z))\n",
    "#from pyspark.sql.types import IntegerType\n",
    "#convertUDF = udf(lambda z: convertCase(z), StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using UDF with DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Using UDF with PySpark DataFrame select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "|ID |Name           |\n",
      "+---+---------------+\n",
      "|1  |Ali Mohamed    |\n",
      "|2  |Saadya Mahmoud |\n",
      "|3  |Mohamed Mahmoud|\n",
      "|4  |Aliaa Eldemery |\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import sql functions for col and other functions we might need later\n",
    "from pyspark.sql import functions as F\n",
    "# using select\n",
    "df.select(F.col(\"ID\"), \\\n",
    "    convertUDF(F.col(\"Name\")).alias(\"Name\") ) \\\n",
    "   .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Using UDF with PySpark DataFrame withColumn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create a new function to reverse the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverseOrder(str):\n",
    "    reStr= str[::-1]\n",
    "    return reStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "|ID |Name           |Reversed Name  |\n",
      "+---+---------------+---------------+\n",
      "|1  |Ali mohamed    |  demahom ilA  |\n",
      "|2  |saadya Mahmoud |duomhaM aydaas |\n",
      "|3  |Mohamed mahmoud|duomham demahoM|\n",
      "|4  |aliaa eldemery |yremedle aaila |\n",
      "+---+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverseOrderUDF = udf(lambda z:reverseOrder(z))   \n",
    "\n",
    "df.withColumn(\"Reversed Name\", reverseOrderUDF(F.col(\"Name\"))) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Registering PySpark UDF & use it on SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use convertCase() function on PySpark SQL, you need to register the function with PySpark by using spark.udf.register()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "|ID |Name_Corrected |\n",
      "+---+---------------+\n",
      "|1  |Ali Mohamed    |\n",
      "|2  |Saadya Mahmoud |\n",
      "|3  |Mohamed Mahmoud|\n",
      "|4  |Aliaa Eldemery |\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"convertUDF2\", convertCase,StringType())\n",
    "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
    "spark.sql(\"select ID, convertUDF2(Name) as Name_Corrected from NAME_TABLE\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating UDF using annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with just a single step by using annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "|ID |Name           |Cureated Name  |\n",
      "+---+---------------+---------------+\n",
      "|1  |Ali mohamed    |ALI MOHAMED    |\n",
      "|2  |saadya Mahmoud |SAADYA MAHMOUD |\n",
      "|3  |Mohamed mahmoud|MOHAMED MAHMOUD|\n",
      "|4  |aliaa eldemery |ALIAA ELDEMERY |\n",
      "+---+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(returnType=StringType()) \n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "df.withColumn(\"Cureated Name\", upperCase(F.col(\"Name\"))) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can try to not use the annotation to test the expected error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "|ID |Name           |Cureated Name  |\n",
      "+---+---------------+---------------+\n",
      "|1  |Ali mohamed    |ali mohamed    |\n",
      "|2  |saadya Mahmoud |saadya mahmoud |\n",
      "|3  |Mohamed mahmoud|mohamed mahmoud|\n",
      "|4  |aliaa eldemery |aliaa eldemery |\n",
      "+---+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# comment the next line and check the result\n",
    "@udf(returnType=StringType()) \n",
    "def lowCase(str):\n",
    "    return str.lower()\n",
    "\n",
    "df.withColumn(\"Cureated Name\", lowCase(F.col(\"Name\"))) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Execution order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark does not guarantee the order of evaluation of subexpressions meaning expressions are not guarantee to evaluated left-to-right or in any other fixed order. PySpark reorders the execution for query optimization and planning hence, AND, OR, WHERE and HAVING expression will have side effects.\n",
    "\n",
    "So when you are designing and using UDF, you have to be very careful especially with null handling as these results runtime exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Handling null check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|ID |Name        |\n",
      "+---+------------+\n",
      "|1  |john jones  |\n",
      "|2  |tracey smith|\n",
      "|3  |amy sanders |\n",
      "|4  |null        |\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"ID\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|_nullsafeUDF(Name)|\n",
      "+------------------+\n",
      "|John Jones        |\n",
      "|Tracey Smith      |\n",
      "|Amy Sanders       |\n",
      "|UNKNOWN           |\n",
      "+------------------+\n",
      "\n",
      "+---+------------+\n",
      "|ID |Name        |\n",
      "+---+------------+\n",
      "|1  |John Jones  |\n",
      "|2  |Tracey Smith|\n",
      "|3  |Amy Sanders |\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"UNKNOWN\" , StringType())\n",
    "\n",
    "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "spark.sql(\"select ID, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
    "          \" where Name is not null\") \\\n",
    "     .show(truncate=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "thank you\n",
    "\n",
    "Mohamed Fakhruldeen,\n",
    "\n",
    "fakhruldeen@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references:\n",
    "    \n",
    "    https://docs.databricks.com/spark/latest/spark-sql/udf-python.html\n",
    "    http://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/udf.html\n",
    "    https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/\n",
    "    https://medium.com/analytics-vidhya/user-defined-functions-udf-in-pyspark-928ab1202d1c\n",
    "    https://www.bmc.com/blogs/how-to-write-spark-udf-python/\n",
    "    https://changhsinlee.com/pyspark-udf/\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
